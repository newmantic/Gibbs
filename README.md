# Gibbs


Gibbs Sampling is a Markov Chain Monte Carlo (MCMC) algorithm used to generate samples from a joint probability distribution, especially when direct sampling is difficult. It is particularly useful when dealing with high-dimensional distributions.


Joint Probability Distribution:
Suppose we have a set of random variables X = (X_1, X_2, ..., X_n) that have a joint probability distribution P(X_1, X_2, ..., X_n).
The goal of Gibbs Sampling is to generate samples from this joint distribution.

Conditional Distribution:
Instead of sampling from the joint distribution directly, Gibbs Sampling samples each variable X_i from its conditional distribution given the current values of all other variables:
P(X_i | X_1, ..., X_(i-1), X_(i+1), ..., X_n)
This conditional distribution represents the probability of X_i given the other variables.


The Gibbs Sampling algorithm iteratively samples each variable from its conditional distribution, using the most recent values of the other variables.

Initialization:
Start with an initial value for each variable. Let's denote this initial state as X^(0) = (X_1^(0), X_2^(0), ..., X_n^(0)).

Iterative Sampling:
At iteration t, update each variable in turn by sampling from its conditional distribution, given the current values of all other variables:
X_1^(t) ~ P(X_1 | X_2^(t-1), X_3^(t-1), ..., X_n^(t-1))
X_2^(t) ~ P(X_2 | X_1^(t), X_3^(t-1), ..., X_n^(t-1))
...
X_n^(t) ~ P(X_n | X_1^(t), X_2^(t), ..., X_(n-1)^(t))
Here, X_i^(t) represents the value of X_i at iteration t.

Convergence:
After many iterations, the sequence of samples (X^(t)) converges to the stationary distribution, which is the joint distribution P(X_1, X_2, ..., X_n).

Burn-In Period:
The initial samples generated by Gibbs Sampling may be biased by the initial values. To reduce this bias, it is common to discard the first k samples (where k is known as the burn-in period).

